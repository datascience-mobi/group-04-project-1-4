---
title: "Linear regression"
author: "Elias Benjamin Farr"
date: "17 Mai 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

We want to predict Ceres scores of our drivermutations ("ERBB2","MYCBP", "PARP10", "PIK3CA") based on data we got from second site targets wright ?
for this we need a data frame for every drivermutation, consisting of ceres scores from our SSTs. 
rownames -> SSTs
colnames -> BCCL harvesting this driver mutation ( we can discuss this :))

#Elias´idea
```{r}
SSTs <- c(rownames(finalFrame)) #could be any list of SSTs
LinResdf0 <- BCCL_kd.ceres[SSTs,] #df, consisting 28 col, nSTTs rows
# something like that: drivmutCL <- c(BCCL_Mutation["Hugosymbol"" == "ERBB2" ])
LinResdf1 <- subset(LinResdf0, colname == drivmutCL)


```
-----------------------------------------------------------------------------------

## Objective

  + For the regression model, we want to predict the CERES score of the driver mutation (dependent variable) based on the CERES score of the SST candidates (independent variables).
  + A multilinear regression will be designed:

## Step 1: Building the LR-Datamatrices
# Load data

  + We now obtained potential SSTs for the four driver mutation. To form a base for the Multilinear regression, we 
    1. Form a vector including the SST rownames
    2. Integrate the CERES scores for the mutations for the linear regression.
    

#UNDER CONSTRUCTION: Need help to automate this function for all 4 datasets

Requiered dataset: "SSTs" # collected names of SST candidates including label of driver mutation

#I did not find a automated approach yet, so doing it caveman-style
  1. Create a dataframe, consisting of 28 col, nSTTs rows
  2. select driver mutation as our dependent variable
  3. obtain column 1 with the driver mutation and other containing CERES values for SSTs
  
```{r}
driver_mut <- c("ERBB2","MYCBP", "PARP10", "PIK3CA") #Input driver mutations
SST_candidatesdf1 <- c(rownames(SST_cand_ERBB2))
SST_candidatesdf2 <- c(rownames(SST_cand_MYCBP))
SST_candidatesdf3 <- c(rownames(SST_cand_PARP10))
SST_candidatesdf4 <- c(rownames(SST_cand_PIK3CA)) # collect all names of SST candidates

# for ERBB2
LinRes_ERBB2 <- BCCL_kd.ceres[SST_candidatesdf1,] 
LinRes_ERBB2 <- t(LinRes_ERBB2)
Driver_ERBB2 <- BCCL_kd.ceres[c("ERBB2"), ]
Driver_ERBB2 <- t(Driver_ERBB2)
LinRes_ERBB2 <- cbind(Driver_ERBB2, LinRes_ERBB2)

#for MYCBP
LinRes_MYCBP <- BCCL_kd.ceres[SST_candidatesdf2,]
LinRes_MYCBP <- t(LinRes_MYCBP)
Driver_MYCBP <- BCCL_kd.ceres[c("MYCBP"), ]
Driver_MYCBP <- t(Driver_MYCBP)
LinRes_MYCBP <- cbind(Driver_MYCBP, LinRes_MYCBP)

#for PARP10
LinRes_PARP10 <- BCCL_kd.ceres[SST_candidatesdf3,]
LinRes_PARP10 <- t(LinRes_PARP10)
Driver_PARP10 <- BCCL_kd.ceres[c("PARP10"), ]
Driver_PARP10 <- t(Driver_PARP10)
LinRes_PARP10 <- cbind(Driver_MYCBP, LinRes_PARP10)

#for PIK3CA
LinRes_PIK3CA <- BCCL_kd.ceres[SST_candidatesdf4,]
LinRes_PIK3CA <- t(LinRes_PIK3CA)
Driver_PIK3CA <- BCCL_kd.ceres[c("PIK3CA"), ]
LinRes_PIK3CA <- t(LinRes_PIK3CA)
LinRes_PIK3CA <- cbind(LinRes_PIK3CA, LinRes_PIK3CA)
```

#generate matrices containing CERES scores of DV and corresponding SSTs
```{r}
#reformat BCCL CERES matrix
BCCL_kd.ceres_df <- as.data.frame(t(BCCL_kd.ceres))
rownames(BCCL_kd.ceres_df) <- tPatients_ID
colnames(BCCL_kd.ceres_df) <- rownames(BCCL_kd.ceres)

#generate a dataframe containing data for each DV
linRegData_ERBB2 <- BCCL_kd.ceres_df %>% select("ERBB2", rownames(SST_cand_ERBB2))
linRegData_MYCBP <- BCCL_kd.ceres_df %>% select("MYCBP", rownames(SST_cand_MYCBP))
linRegData_PARP10 <- BCCL_kd.ceres_df %>% select("PARP10", rownames(SST_cand_PARP10))
linRegData_PIK3CA <- BCCL_kd.ceres_df %>% select("PIK3CA", rownames(SST_cand_PIK3CA))
```

#Step 2: Data Preperation

To test the resulting Linear Regression model, we will split the datset into 75:25 training:test data.
  1. We check normal distribution by a qq-Plot
  2. Split the dataset into 75:25 training:test data
```{r}
create_qqplot <- function(x){
                        qqnorm(x, main = deparse(substitute(x)))
                        qqline(x)} 
#introducing a function creating qq-plots, check if normality is approximated; the "deparse(substitute(x))" assigns the correesponding dataset name to the qq-plot.

create_qqplot(LinRes_ERBB2)
create_qqplot(LinRes_MYCBP)
create_qqplot(LinRes_PARP10)
create_qqplot(LinRes_PIK3CA) #perform qqplot creation

data_split <- function(x){
                        trainingData <- x[c(1:0.75*nrow(x)), c(1:ncol(x))] # define model training data
                        testData  <- x[c((0.75*nrow(x)+1):nrow(x)), c(1:ncol(x)) ]# define test training data 
} # introducing a function splitting the data into 75:25 training:test data
  # still not working

data_split(LinRes_ERBB2)
data_split(LinRes_MYCBP)
data_split(LinRes_PARP10)
data_split(LinRes_PIK3CA) #perform data split 75:25 for all datasets

```

#Step 3: The regression model of training data

  1. Define the function, generally described as:
    + y = a + b1x1 + b2x2 + (...) + bnxn
  2. Receive intercept and complete function
  3. Evaluate model by summary()-Function
```{r}
LinRes_ERBB2 <- as.data.frame(LinRes_ERBB2)
LinRes_MYCBP <- as.data.frame(LinRes_MYCBP)
LinRes_PARP10 <- as.data.frame(LinRes_PARP10)
LinRes_PIK3CA <- as.data.frame(LinRes_PIK3CA) #safe as data frame, otherwise no error reported

---------------------------------------------------------------------------------------------
testData <- as.data.frame(testData) #to run the code this format-change is required, otherwise: error reported
model1 <- lm(ERBB2 ~ BCL9L + C10orf95 + CREBBP + FAM83C + FBN3 + HEATR2 + KIAA1524 + KLHL40 + MPP3 + MYL9 + NCR2 + NLRP13 + POTEH + RNF181 + STXBP1, data = LinResdf0.df)
summary(model1)
```
Now, we take a look at
  + F-Statistic look if those are significant, meaning that judges on multiple coefficients taken together at the same time, rather than doing a t test for all individual variables.
    + H0: The fit of intercept only model and the current model is same. i.e. Additional variables do not provide value taken together
    + H1: The fit of intercept only model is significantly less compared to our current model. i.e. Additional variables do make the model significantly better.
```{r}
#UNDER CONSTRUCTION: Function printing results of F-test as "H0 or H1 accepted"
qf(.95, df1=15, df2=12) 
summary(model1)$coefficient
coeffdf1 <- as.data.frame(summary(model1)$coefficient)

distPred <- predict(lmMod, testData)
confint(model1)
```

# Step 4: Run with test data
Now, we run the test data to obtain actual predictions
For further evaluation of model fit, we will
  + calculate Correlation Coefficient. We use the Spearman correlation.
  + Create a density plot to check if predicted and actual values overlap
  
```{r}
model1_Pred <- predict(model1, testData)
summary (model1)

actuals_preds1 <- data.frame(cbind(actuals=testData, predicteds=model1_Pred)) #build dataframa for easier calculation of correlation coeff.
View(actuals_preds1)

cor(actuals_preds[1:7, 1:18], actuals_preds[1:7, 19], method = c("spearman")) #calculate spearman corr. coeff. between observed and predicted values - I feel like I made a mistake in construction (Pls help)
corrmodel1 <- as.matrix(cor(actuals_preds[1:7, 1:18], actuals_preds[1:7, 19], method = c("spearman"))) # format correlation as matrix to seogn a heatmap for further use

res <- cor(actuals_preds)

install.packages("corrplot")
library(corrplot)
corrplot(res, type = "upper", order = "hclust", 
         tl.col = "black", tl.srt = 45) # visualize correlation 

-----------------------------------------------------------------------------------  
## Ideas collected from:
[linked phrase](https://www.dataquest.io/blog/statistical-learning-for-predictive-modeling-r/)
[linked phrase](http://r-statistics.co/Linear-Regression.html#Predicting%20Linear%20Models)
