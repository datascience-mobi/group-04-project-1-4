---
title: "Linear regression"
author: "Elias Benjamin Farr"
date: "17 Mai 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

We want to predict Ceres scores of our drivermutations ("ERBB2","MYCBP", "PARP10", "PIK3CA") based on data we got from second site targets wright ?
for this we need a data frame for every drivermutation, consisting of ceres scores from our SSTs. 
rownames -> SSTs
colnames -> BCCL harvesting this driver mutation ( we can discuss this :))

#Elias´idea
```{r}
SSTs <- c(rownames(finalFrame)) #could be any list of SSTs
LinResdf0 <- BCCL_kd.ceres[SSTs,] #df, consisting 28 col, nSTTs rows
# something like that: drivmutCL <- c(BCCL_Mutation["Hugosymbol"" == "ERBB2" ])
LinResdf1 <- subset(LinResdf0, colname == drivmutCL)


```
-----------------------------------------------------------------------------------

## Objective

  + For the regression model, we want to predict the CERES score of the driver mutation (dependent variable) based on the CERES score of the SST candidates (independent variables).
  + A multilinear regression will be designed:

## Step 1: Building the LR-Datamatrices
# Load data

  + We now obtained potential SSTs for the four driver mutation. To form a base for the Multilinear regression, we 
    1. Form a vector including the SST rownames
    2. Integrate the CERES scores for the mutations for the linear regression.
    

#UNDER CONSTRUCTION: Need help to automate this function for all 4 datasets

Requiered dataset: "SSTs" # collected names of SST candidates including label of driver mutation

#I did not find a automated approach yet, so doing it caveman-style
  1. Create a dataframe, consisting of 28 col, nSTTs rows
  2. select driver mutation as our dependent variable
  3. obtain column 1 with the driver mutation and other containing CERES values for SSTs
  
```{r}
driver_mut <- c("ERBB2","MYCBP", "PARP10", "PIK3CA") #Input driver mutations
SST_candidatesdf1 <- c(rownames(SST_cand_ERBB2))
SST_candidatesdf2 <- c(rownames(SST_cand_MYCBP))
SST_candidatesdf3 <- c(rownames(SST_cand_PARP10))
SST_candidatesdf4 <- c(rownames(SST_cand_PIK3CA)) # collect all names of SST candidates

# for ERBB2
LinRes_ERBB2 <- BCCL_kd.ceres[SST_candidatesdf1,] 
LinRes_ERBB2 <- t(LinRes_ERBB2)
Driver_ERBB2 <- BCCL_kd.ceres[c("ERBB2"), ]
Driver_ERBB2 <- t(Driver_ERBB2)
LinRes_ERBB2 <- cbind(Driver_ERBB2, LinRes_ERBB2)

#for MYCBP
LinRes_MYCBP <- BCCL_kd.ceres[SST_candidatesdf2,]
LinRes_MYCBP <- t(LinRes_MYCBP)
Driver_MYCBP <- BCCL_kd.ceres[c("MYCBP"), ]
Driver_MYCBP <- t(Driver_MYCBP)
LinRes_MYCBP <- cbind(Driver_MYCBP, LinRes_MYCBP)

#for PARP10
LinRes_PARP10 <- BCCL_kd.ceres[SST_candidatesdf3,]
LinRes_PARP10 <- t(LinRes_PARP10)
Driver_PARP10 <- BCCL_kd.ceres[c("PARP10"), ]
Driver_PARP10 <- t(Driver_PARP10)
LinRes_PARP10 <- cbind(Driver_MYCBP, LinRes_PARP10)

#for PIK3CA
LinRes_PIK3CA <- BCCL_kd.ceres[SST_candidatesdf4,]
LinRes_PIK3CA <- t(LinRes_PIK3CA)
Driver_PIK3CA <- BCCL_kd.ceres[c("PIK3CA"), ]
LinRes_PIK3CA <- t(LinRes_PIK3CA)
LinRes_PIK3CA <- cbind(LinRes_PIK3CA, LinRes_PIK3CA)
```

#generate matrices containing CERES scores of DV and corresponding SSTs
```{r}
#reformat BCCL CERES matrix
BCCL_kd.ceres_df <- as.data.frame(t(BCCL_kd.ceres))
rownames(BCCL_kd.ceres_df) <- tPatients_ID
colnames(BCCL_kd.ceres_df) <- rownames(BCCL_kd.ceres)

#generate a dataframe containing data for each DV
linRegData_ERBB2 <- BCCL_kd.ceres_df %>% select("ERBB2", rownames(SST_cand_ERBB2))
linRegData_MYCBP <- BCCL_kd.ceres_df %>% select("MYCBP", rownames(SST_cand_MYCBP))
linRegData_PARP10 <- BCCL_kd.ceres_df %>% select("PARP10", rownames(SST_cand_PARP10))
linRegData_PIK3CA <- BCCL_kd.ceres_df %>% select("PIK3CA", rownames(SST_cand_PIK3CA))
```

#Step 2: Data Preperation

To test the resulting Linear Regression model, we will split the datset into 75:25 training:test data.
  1. We check normal distribution by a qq-Plot
  2. Split the dataset into 75:25 training:test data
```{r}
create_qqplot <- function(x){
                        qqnorm(x, main = deparse(substitute(x)))
                        qqline(x)} 
#introducing a function creating qq-plots, check if normality is approximated; the "deparse(substitute(x))" assigns the correesponding dataset name to the qq-plot.

create_qqplot(LinRes_ERBB2)
create_qqplot(LinRes_MYCBP)
create_qqplot(LinRes_PARP10)
create_qqplot(LinRes_PIK3CA) #perform qqplot creation
```

## Define a big function doing datasplit, Model training and testing all in one
To run the Multilinear Regression, it is advised to create a long function which
  1. Splits the dataset in 75:25 training:test data
    + Previous installation of the "caTools" package is necessary
  2. Conducts Training of Regression model with training data
    + predict Driver mutation CERES (y; dep.variable) from SST CERES scores (x1 ... xn; indep.variables)
  3. Runs Test Data on model for performance evaluation
  4. Calculates Spearman correlation to further asess model performance
    + Visualizes by barplot
```{r}
install.packages("caTools")
library(caTools) #necessary packages

MultiLinReg <- function(x){
                          DependentVariable <- colnames(x[1]) #The dependent variable is always the driver mutation
                          split = sample.split(x["DependentVariable"], SplitRatio = 0.75) #split the dataset into 3/4 Training and 1/4 Testing dataset
                          training_set = subset(x, split == TRUE)
                          test_set = subset(x, split == FALSE)
                          
                          regressor = lm(formula = DependentVariable ~ ., data=training_set)
                          #predict CERES of Driver Mutation based on all (=.) the input variables (SSTs)
                          
                          y_predict = predict(regressor, newdata = test_set) #predict the Driver mut. CERES score based on the test data
                          test_set$Prediction = y_predict #adding predictions to the dataset
                          test_set #Now you can compare your Predictions (last column) with the real values of the startups (2nd last column)
                          
                          cordf <- cor.test(test_set$DependentVariable, test_set$Prediction, method = spearman) #to judge model performance, calculate Spearman corr. between predicted and observed values
                          barplot(cordf, main = deparse(substitute(x))) #create a barplot for correlation visualization and further evaluation
}

MLR_files <- c("linRegData_ERBB2", "linRegData_MYCBP", "linRegData_PARP10", "linRegData_PIK3CA")
lapply(MLR_files, MultiLinReg)

# CURRENT ISSUE: "Error in terms.formula(formula, data = data) : '.' erscheint in der Formel und 'data' Argument ist ungültig"
```



----------------------------------------------------------------------------------------------
# Older Notes we could keep for a while
```{r}
data_split <- function(x){
                        trainingData <- x[1:0.75*nrow(x), 1:ncol(x)] 
                        testData  <- x[(0.75*nrow(x)+1):nrow(x), 1:ncol(x) ]
                        trainingData <- as.data.frame(trainingdata)
                        testData  <- as.data.frame(testData)
                        return(trainingData)
                        return(testData)
}  #define model training and test data by a function
  # still not working

data_split(LinRes_ERBB2)
data_split(LinRes_MYCBP)
data_split(LinRes_PARP10)
data_split(LinRes_PIK3CA) #perform data split 75:25 for all datasets

```
#Step 3: The regression model of training data

  1. Define the function, generally described as:
    + y = a + b1x1 + b2x2 + (...) + bnxn
  2. Receive intercept and complete function
  3. Evaluate model by summary()-Function
```{r}
LinRes_ERBB2 <- as.data.frame(LinRes_ERBB2)
LinRes_MYCBP <- as.data.frame(LinRes_MYCBP)
LinRes_PARP10 <- as.data.frame(LinRes_PARP10)
LinRes_PIK3CA <- as.data.frame(LinRes_PIK3CA) #safe as data frame, otherwise no error reported

create_MLR <- function(x){
                      y <- x[,1]
                      model <- lm(y ~ ., data = x ) #The "."-identifier contains all variables not yet mentioned in the formula (2:ncol(x)) in our case. This makes modelling much easier.
                      summary(model)
} #Creates a multilinear regression, using the first column (DV) as y-variable.

create_MLR(LinRes_ERBB2)
create_MLR(LinRes_MYCBP)
create_MLR(LinRes_PARP10)
create_MLR(LinRes_PIK3CA)

# CURRENT ISSUE: x Variables include Driver; thus R squared and F Test not ready for interpretation
```

# Step 4: Model evaluation
  1. F-Statistic judging simultaneously on multiple coefficients taken together, rather than doing a t test for all individual variables:
    + select 0.05 as significance value
    + H0: The fit of intercept only model and the current model is same, meaning that the additional variables do not provide value taken together
    + H1: The fit of intercept only model is significantly less compared to our current model, meaning that additional variables do improve the model significantly.
    + if H1 = true, we can be confident to have a good R-squared value
  2. It is recommended to do a residual (scatter) plot, to check the type of residual distribution
  
```{r}
residual.plot(Expected, Residuals, sigma, main = deparse(substitute(Expected)), 
  col.pts = "blue", col.ctr = "red", col.sgm = "black", cex = 0.5, 
  gray.scale = FALSE, xlab = "Predicted", ylab = "Residuals") #something like this
```

----------------  
## Ideas collected from:
[linked phrase](https://www.dataquest.io/blog/statistical-learning-for-predictive-modeling-r/)
[linked phrase](http://r-statistics.co/Linear-Regression.html#Predicting%20Linear%20Models)
