---
title: "Linear regression"
author: "Elias Benjamin Farr"
date: "17 Mai 2019"
output: tufte::tufte_handout
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
### Multilinear Regression
## Objective

  + For the regression model, we want to predict the CERES score of the driver mutation (dependent variable) based on the CERES score of the SST candidates (independent variables).
  + A multilinear regression will be designed:
  
# Packages
```{r}
install.packages("ggpubr")
library("ggpubr")
```

# Step 1: Construct datasets for Regression
  + Multilinear regression, we 
    1. reform the BCCL CERES matrix
      + By transponing and format change, we obtain a matrix with colnames = Gene names and rownames = Cell line indices (equal to tPatients_ID)
    2. Collect the relevant the CERES scores for the mutations for the linear regression.
      + All columns with the corresponding names of the SST genes plus the driver mutation will be collected and copied into a new dataframe
      
```{r}
#reformat BCCL CERES matrix
BCCL_kd.ceres_df <- as.data.frame(t(BCCL_kd.ceres))
rownames(BCCL_kd.ceres_df) <- tPatients_ID
colnames(BCCL_kd.ceres_df) <- rownames(BCCL_kd.ceres)

#generate a dataframe containing data for each DV
linRegData_ERBB2 <- BCCL_kd.ceres_df %>% select(rownames(SST_Data$ERBB2), "ERBB2")
linRegData_MYCBP <- BCCL_kd.ceres_df %>% select(rownames(SST_Data$MYCBP), "MYCBP")
linRegData_PARP10 <- BCCL_kd.ceres_df %>% select(rownames(SST_Data$PARP10), "PARP10")
linRegData_PIK3CA <- BCCL_kd.ceres_df %>% select(rownames(SST_Data$PIK3CA), "PIK3CA")

data_regression <- list(linRegData_ERBB2, linRegData_MYCBP, linRegData_PARP10, linRegData_PIK3CA)
head(data_regression)
```

#Step 2: Checking assumptions
To run regression, certain conditions must be fulfilled:
  + Check Normality with a qqPlot
    + Using the aes-function for aesthetic plots, provided by the "ggplot2" package
  + Check Multicollinearity
    + Multicollinearity is given if two dependent variables are strongly correlated, which can lead to difficulties in further analysis
    + As rule of thumb, the Pearson correlation coefficient should be lower than 0.8:
      $r < 0.80$
    + if this requirement is not met, one of the correlated variables should be deleted as it is seen as redundant.
  + Run Shapiro-Wilk Test
    + hypothesis test with H0: data is normal distributed
      + we want to observe a p-value > 0.05
    
```{r}
lapply(seq_along(data_regression), function(a){  
  p <- ggplot(data_regression[[a]], aes(sample=data_regression[[a]][,ncol(data_regression[[a]])])) + stat_qq()+ stat_qq_line() #create qq-Plot with line for comparison to normal distribution
   p + labs(title = "QQ-Plot", subtitle = "Check if data is approx. normally distributed")
  
  r <-  round(cor(data_regression[[a]]),2) #create table with correlation values between all SSTs; the values are rounded to two decimal places [,2)]
  
  return(list(p,r))
   })

lapply(seq_along(data_regression), function(a){
  as.numeric(data_regression[[a]])
  s <- shapiro.test(data_regression[[a]]) #to expand quality control, run Shapiro Test
  return(s)
})

#Error  object cannot be coerced to type 'double'
#need to save data as.mumeric, won´t work yet.
```
  + Quick Results:
    + Data of MYCBP and PARP10 is approx. normally distributed; ERBB2 and PIK3CA not
    + no multicollinearity detected, all 0.80 > x
    + 


#Step 3: Multilinear Regression: Data split, Model calculation, Return correlation data for evaluation
To run the Multilinear Regression, it is advised to create a long function which
  1. Splits the dataset in __75:25__ training:test data
    + Previous installation of the "caTools" package is necessary
  2. Conducts Training of Regression model with training data
    + predict Driver mutation CERES (y; dep.variable) from SST CERES scores (x1 ... xn; indep.variables)
  3. Runs Test Data on model for performance evaluation
  4. Calculates Spearman correlation to further asess model performance
  
```{r}
input_data <- data_regression

lapply(seq_along(input_data), function(a) {
  set.seed(123) #initialize the random numbers, better reproducable
  data <- input_data[[a]] #get the data
  colnames(data)[length(colnames(data))] <- "Predictor"
  
  split = sample.split(data[,ncol(data)], SplitRatio = 0.75) #split the dataset into 3/4 Training and 1/4 Testing dataset
  training_set = subset(data, split == TRUE) #use the labels to get the training data
  test_set = subset(data, split == FALSE)

  regressor = lm(formula = Predictor ~ ., 
                 data = training_set)  #predict CERES of Driver Mutation based on all (=.) the input variables (SSTs)
  
  ResidPlot <- hist(resid(regressor),main='Histogram of residuals',xlab='Standardised
Residuals',ylab='Frequency') #plot a histogram of standardised residuals to check the assumption of normality
  HomoPlot <- plot(regressor, which = 1) #fitted values and residuals plot to check the assumption of homoscedasticity
 
### Gibt nur jeweils 1 statt 4 Plots raus, wie kann ich das verbessern?
  
  y_pred = predict(regressor, newdata = test_set) #predict the Driver mut. CERES score based on the test data
  test_set$Prediction = y_pred #adding predictions to the dataset
  
  corVal <- cor.test(test_set$Predictor , test_set$Prediction, method = "spearman") #to judge model performance, calculate Spearman corr. between predicted and observed values

 SpearmanPlot <- ggscatter(test_set, x = "test_set$Predictor",
                   y = "test_set$Prediction",
                   add = "reg.line", #add the regression line
                   cor.coef = TRUE,
                   cor.method = "spearman",#output is Spearman correlation coeff.
                   conf.int = TRUE, #show confidence interval
                   xlab = "Observed CERES score",
                   ylab = "Predicted CERES score") #name x/y axis 
 
 ###Error in `[.data.frame`(data, , x) : nicht definierte Spalten gewählt
 
 return (list(ResidPlot, HomoPlot, corVal, SpearmanPlot))
})


```
https://stackoverflow.com/questions/45602586/undefined-columns-data-frame-error
https://github.com/kassambara/ggpubr/issues/23 #this doesn´t help


## Interpretation of Spearman correlation coefficient

# Revision: Meaning of the correlation coefficient rho
  + the rank-correlation coefficient can take values from +1 to -1
    + rho ~ 1: perfect positive association of ranks
    + rho ~ 0: no association of ranks
    + rho ~ -1: perfect negative association of ranks

# Revision: Adding a statistical parameter
  + To further interprete the p-value, we construct a H0 / H1 hypothesis:
    + H0: There is no association between the predicted and observed variable.
    + H1: There is an association between the predicted and observed variable.
  + Most importantly, it is not possible to assess the strength of the correlation based on the p-value. It rather judges the statistical test than the resulting value.
    + as default, we set the significance level to 0.05

# Objective
 + We will the compare this values for the different models
  + For a positive correaltion, two variables move in the same direction.
    + For a well-designed model, we expect the precicted and observed CERES scores being approx. even
  + Thus, optimal model design is achieved if:
      + the correlation coefficient is close to 1
      + the p-value < 0.05
    

# Results
We obtained

Model     | p-value     | rho      | 
----------|-------------|----------|
ERBB2     | 0.8397      | 0.1071   |
MCYBP     | 0.7825      |-0.1429   |
PARP      | 0.1095      |-0.6786   |
PIK3CA    | 0.7131      |-0.1786   |

# Discussing p-Values and correlation coefficients

  + ERBB2-model:
    + Shows the highest rho value of all four models, but ~ 0.1 is a rather weak positive relationship
    + The p-value leads to __acceptance of H0__.
    + The data is not even approx. normal distributed.
    
  + MYCBP-model:
    + The rho value leads to very weak negative correlation.
    + A p-value of ~ 0.78 leads to condifent acceptance of H0.
    + Data is approx. normal distributed.
    
  + PARP10-model:
    + The rho value of ~ -0.67 is a moderate negative relationship.
    + The p-value leads to acceptance of H0. The p-value is the lowest with ~ 0.11. But even if we would raise the confidence level to 0.10 (which would be a questionable move in statistics), the result wouldn´t be significant to accept H1.
    + Data is approx. normal distributed.
    
  + PIK3CA-model:
    + The rho value leads to very weak negative correlation.
    + A p-value of ~ 0.78  leads to condifent acceptance of H0.

  + __All in all, all multilinear regression models failed to deliver a p-value of significant relevance. Thus, they should not be used for further predictions.__
    + The models for MYCBP and PIK3CA only showed weak, negative correlation, which is not useful for further work with this model.
    + The PARP10 model resulted in a moderate negative relationship, which is not of relevance for our objective.
    + The ERBB2 model did not deliver a significant p-value, but indeed contains at least a weak positive correlation.

### Discussing possible Reasons for the unsatisfying results
## Possible causes in the Regression model
#Sample Size

It seems possible that the small sample size for the regression model lead to the lack of significant results, with only 28 events (CERES score observed in cell line) per predicting variable (SST gene).

But, a common thumb-rule for building a regression model is to calculate the _limiting sample size_:

 $p < \frac{m}{15}$
  + p: number of predictor variables
  + m: limiting sample size; for continuous response variables it is the  total sample size n

If this equation is fulfilled, the model is supposed to be reliable.(Harrell 2001)
In fact, this condition is true for all four models.

## Checking assumptions
# Distribution

Checking distributional assumptions (= normal distribution) is also important and a different model should be chosen if needed. (Harrell 2001)
  + As shown in the qq-Plots, normal distribution does only apply to the models of MYCBP and PARP10
  + ERBB2 on the other hand, delivers the highest rho, but is in no way approx. normal distributed.
  + The PIK3CA model also does not fulfill the condition of normal distribution.

The normal distribution does not lead to higher correlation coefficients. But not satisfying distribution assumptions should be treated as a red flag; and the model is not suited for further use.

There was no multicollinearity detected. The condition r < 0.8 was met for all variables.

## Possible causes in the Prediction model
#Training & prediction data size
Our data seems to have an appropriate size for a basic regression model.
On the other hand, for the supervised machine learning process of the regression, sample sizes up to n = 50,000 are reported.(Libbrecht and Noble 2015) 
In general, more data will lead to improved data quality. Generally, scientists are advised to focus on obtaining and analyzing larger data sets instead of comparing different learning techniques on small training sets.(Banko and Brill 2001)


--------------------
  + Banko, M. and E. Brill (2001). Scaling to very very large corpora for natural language disambiguation. Proceedings of the 39th Annual Meeting on Association for Computational Linguistics. Toulouse, France, Association for Computational Linguistics: 26-33.
  + Harrell, F. E. (2001). Regression modeling strategies : with applications to linear models, logistic regression, and survival analysis. New York ; Berlin ; Heidelberg [u.a.], Springer.
  + Libbrecht, M. W. and W. S. Noble (2015). "Machine learning applications in genetics and genomics." Nature reviews. Genetics 16(6): 321-332.





  